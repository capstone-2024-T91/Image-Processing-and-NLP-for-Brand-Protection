{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/youssefchouay/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable weights and biases\n",
    "import os\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_dtype = getattr(torch, \"float16\")\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type='nf4',\n",
    "#         bnb_4bit_compute_dtype=compute_dtype,\n",
    "#         bnb_4bit_use_double_quant=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../data/processed/phishing_train_llm.csv\", \"test\": \"../data/processed/phishing_test_llm.csv\"})\n",
    "# Load the model Phi-2\n",
    "model_name='microsoft/phi-2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM(model_name,device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instruct: Classify the following body as \"Phishing Email\" or \"Safe Email\".\n",
      "Body: Did you know? You can refinance up to 1 time per year.\n",
      "Stop fighting for lenders let them fight for you! \n",
      "Make them work for your business by giving you the lowest rates around!\n",
      "Please fill out 30 second insta-quote and monthly savings calculation\n",
      "http://skeletosyard.com\n",
      "Response:\n",
      "Phishing Email\n",
      "\n",
      "Phishing Email\n"
     ]
    }
   ],
   "source": [
    "# Test the Model with Zero Shot Inferencing\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "index = 20\n",
    "\n",
    "body = dataset['test'][0]['body']\n",
    "label = dataset['test'][0]['label']\n",
    "prompt = f\"\"\"\n",
    "Instruct: Classify the following body as \"Phishing Email\" or \"Safe Email\".\n",
    "Body: {body}\n",
    "Response:\\n\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "res = model.generate(**inputs, max_new_tokens=90)\n",
    "translated_text = tokenizer.batch_decode(res, skip_special_tokens=True)[0]\n",
    "print(translated_text)\n",
    "print(label)\n",
    "# print(model.can_generate())\n",
    "# for i in range(index):\n",
    "#     body = dataset['test'][i]['body']\n",
    "#     label = dataset['test'][i]['label']\n",
    "#     prompt = f\"\"\"\n",
    "#                 Instruct: Classify the following body as \"Phishing Email\" or \"Safe Email\".\n",
    "#                 Body: {body}\n",
    "#                 Response:\\n\n",
    "#             \"\"\"\n",
    "#     # Tokenize the prompt\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#     # Generate the response\n",
    "#     res = model.generate(**inputs, max_new_tokens=10)\n",
    "#     print(res)\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with few shot inferencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
